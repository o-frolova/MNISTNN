{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_labels(dataset):\n",
    "    return dataset.data.numpy(), dataset.targets.numpy().reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_data(data):\n",
    "    return data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(data):\n",
    "    return data.reshape(len(data), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(dataset, type_data):\n",
    "    data, labels = split_data_labels(dataset)\n",
    "    data = normalization_data(data)\n",
    "    data = reshape_data(data)\n",
    "    if type_data == 'train':\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        labels = encoder.fit_transform(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = preprocessing_data(mnist_trainset, 'train')\n",
    "test_data, test_labels = preprocessing_data(mnist_trainset, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.weights1 = np.random.randn(input_dim, hidden_dim)\n",
    "        self.bias1 = np.zeros((1, hidden_dim))\n",
    "\n",
    "        self.weights2 = np.random.randn(hidden_dim, output_dim)\n",
    "        self.bias2 = np.zeros((1, output_dim))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "    \n",
    "    def mse_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.hidden_output = self.relu(np.dot(X, self.weights1) + self.bias1)\n",
    "        output_before_softmax = np.dot(self.hidden_output, self.weights2) + self.bias2\n",
    "        self.output = self.softmax(output_before_softmax)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_pass(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        grad_softmax = self.output - y   \n",
    "\n",
    "        grad_weights2 = np.dot(self.hidden_output.T, grad_softmax) / m  \n",
    "        grad_bias2 = np.sum(grad_softmax, axis=0, keepdims=True) / m\n",
    "        \n",
    "        grad_hidden = np.dot(grad_softmax, self.weights2.T) * (self.hidden_output > 0)\n",
    "        \n",
    "        grad_weights1 = np.dot(X.T, grad_hidden) / m\n",
    "        grad_bias1 = np.sum(grad_hidden, axis=0, keepdims=True) / m\n",
    "        \n",
    "        self.weights2 -= learning_rate * grad_weights2\n",
    "        self.bias2 -= learning_rate * grad_bias2\n",
    "        self.weights1 -= learning_rate * grad_weights1\n",
    "        self.bias1 -= learning_rate * grad_bias1\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            output = self.forward_pass(X)\n",
    "            loss = self.mse_loss(output, y)\n",
    "            self.backward_pass(X, y, learning_rate)\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {i}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_output = self.relu(np.dot(X, self.weights1) + self.bias1)\n",
    "        output = self.softmax(np.dot(hidden_output, self.weights2) + self.bias2)\n",
    "        return np.argmax(output, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "input_dim = train_data.shape[1]\n",
    "hidden_dim = 512\n",
    "output_dim = train_labels.shape[1]\n",
    "epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.18481271328466992\n",
      "Epoch 10, Loss: 0.16794339092015162\n",
      "Epoch 20, Loss: 0.1504265760424067\n",
      "Epoch 30, Loss: 0.13430067647974098\n",
      "Epoch 40, Loss: 0.12050285133400418\n",
      "Epoch 50, Loss: 0.1084432929661027\n",
      "Epoch 60, Loss: 0.09874163498215775\n",
      "Epoch 70, Loss: 0.09097147373423747\n",
      "Epoch 80, Loss: 0.08462718676577068\n",
      "Epoch 90, Loss: 0.07934327032502272\n"
     ]
    }
   ],
   "source": [
    "# Network creation and training\n",
    "nn = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "nn.train(train_data, train_labels, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.6179833333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = nn.predict(test_data)\n",
    "print(f'Model accuracy: {accuracy_score(test_labels, y_pred)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
